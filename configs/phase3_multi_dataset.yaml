# Phase 3 Multi-Dataset Training Configuration
# Enhanced training for cross-chain airdrop hunter detection

datasets:
  - type: arbitrum
    data_path: ./data
    kwargs:
      max_sequence_length: 50
      start_date: "2023-03-15"
      end_date: "2023-03-22"
      include_known_hunters: true
      
  - type: jupiter
    data_path: ./data
    kwargs:
      max_sequence_length: 50
      start_date: "2024-01-15" 
      end_date: "2024-01-30"
      anti_farming_analysis: true
      include_stablecoin_farming: true

# Model configuration
model:
  d_model: 256
  temporal_layers: 4
  temporal_heads: 8
  graph_layers: 6
  graph_heads: 8
  fusion_type: cross_attention
  dropout: 0.1
  lr: 1e-4
  weight_decay: 1e-5
  epochs: 50
  verbose: true

# Training configuration  
batch_size: 32
num_workers: 4
device: auto

# Output configuration
output_dir: ./experiments/phase3_multi_dataset

# Cross-validation settings
cross_validation:
  enabled: false
  n_folds: 5
  
# Baseline method configurations
trustalab:
  star_threshold_degree: 10
  tree_max_depth: 5
  chain_min_length: 3
  similarity_threshold: 0.8
  n_estimators: 100
  max_depth: 10
  random_state: 42

subgraph_propagation:
  max_neighbors_l1: 50
  max_neighbors_l2: 100
  input_dim: 64
  hidden_dim: 128
  num_layers: 3
  heads: 4
  dropout: 0.1
  lr: 1e-3
  epochs: 100
  patience: 10
  verbose: true

enhanced_gnns:
  input_dim: 64
  hidden_dim: 128
  num_layers: 3
  heads: 8
  dropout: 0.1
  lr: 1e-3
  epochs: 80
  patience: 15
  verbose: true

traditional_ml:
  # LightGBM configuration
  num_leaves: 31
  learning_rate: 0.1
  feature_fraction: 0.9
  bagging_fraction: 0.8
  bagging_freq: 5
  min_child_samples: 20
  num_boost_round: 100
  early_stopping_rounds: 10
  random_state: 42
  
  # Random Forest configuration
  n_estimators: 100
  max_depth: 15
  min_samples_split: 5
  min_samples_leaf: 2

# Baseline methods to compare
baselines:
  - trustalab
  - subgraph_propagation
  - gat
  - graphsage
  - lightgbm